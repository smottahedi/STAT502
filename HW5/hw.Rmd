---
title: "hw5"
author: "Sam Mottahedi"
date: "July 29, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd('~/Projects/STAT502/HW5/')
```

## 1) 

### a) 

```{r}
data = read.table('marketshare.txt',header=T, colClasses = c('numeric','numeric', 'numeric', 'factor', 'factor', 'numeric')) 

share = data[,1]; A = as.factor(data[,4]); B = as.factor(data[,5]) 
tapply(share,A:B,length)
```

$y_{ijk} = \mu_{..} + \alpha_{i} + \beta_{j} + (\alpha \beta)_{ij} + \epsilon_{ijk}$

$i: 1 \dots 2$

$j: 1 \dots 2$

$k: 1 \dots 8$



### b)

```{r}
fit <- lm(share ~ A*B)
anova(fit)
plot(fit$residuals)
qqnorm(fit$residuals)
```

The result does not show major deviation from normal dist residual and constant variance assumption.

### c)

$y_{ijk} = \mu_{..} + \alpha_{i} + \beta_{j} + \epsilon_{ijk}$

```{r}
fit2 <- lm(share ~ A + B)
anova(fit2)
anova(fit2,fit)
```

$F^* = `r ((0.83063 - 0.78499) / (33 - 32)) / (0.78499/32)`$

$F = `r qf(0.95, 1, 32)`$

$F^* < F$

concluding $H_0$ the interaction term is not significant.

### d) 

$y_{ijk} = \mu_{..} + \beta_{j} + \epsilon_{ijk}$

The full model for this test is the reduced model from part c because the two factor are included in the full model and the reduced model does not include the factor A since we want to test factor A effect.

```{r}
fit3 <- lm(share ~ B)
anova(fit3)
anova(fit2,fit3)
```

$F^* = `r ((2.21870 - 0.83063) / (34 - 33)) / (0.83063/33)`$

$F = `r qf(0.95, 1, 33)`$

$F^* > F$

### e)

$y_{ijk} = \mu_{..} + \beta_{j} + \epsilon_{ijk}$


```{r}
fit4 <- lm(share ~ A)
anova(fit4)
anova(fit2,fit4)
```

$F^* = `r ((0.91672 - 0.83063) / (34 - 33)) / (0.83063/33)`$

$F = `r qf(0.95, 1, 33)`$

$F^*  < F$

concluding $H_0$ the factor B is not a significant effect in promotion package.

## 2)


### a)

Fixed effect model is more appropriate since if we decide to repeat the experiment using the 4 types of retirement as treatments, the new sample exhaust the population. Also, the effect (retirement plan) is the main interest of this study. 
 
### b)

Random effect model is more appropriate since the sample size is small part of the population and repeating the experiment would result in  a new sample each time.


## 3) 

```{r}
data = read.table('coils.dat', header=T, colClasses = c('numeric', 'factor', 'factor')) 
y = data[,1] 
machine = as.factor(data[,2]) 
coil = as.factor(data[,3]) 
```


### a)

$Y_ij = \mu_i + \epsilon_{ij}$

$\mu_i \& \epsilon_{ij}: \textrm{independent random variables}$

$i=1, \dots, 4; j = 1, \dots , 10$

### b)

```{r}
anova(lm(y ~ machine, data))
```


$H_0: \sigma_{\mu}^2 = 0$

$H_a: \sigma_{\mu}^2 > 0$


$F^* = `r 200.83/7.15` > F= `r qf(1-0.1, 3, 36)`$  

we conclude $H_a$, and mean measured characteristic is different for different machines.

### c)

$\bar{Y}_{..} = `r mean(data$y)`$

$s^2\{\bar{Y}_{..}\} = `r 200.83/ (4*10)`$

$s\{\bar{Y}_{..}\} = `r sqrt(200.83/ (4*10))`$

$t(0.95, 3) = `r qt(0.95, 3)`$

$`r mean(data$y) - sqrt(200.83/ (4*10)) * qt(0.95, 3)` \le \mu_. \le `r mean(data$y) + sqrt(200.83/ (4*10)) * qt(0.95, 3)`$


### d) 

$\sigma^2_{\mu} = `r (200.83 -7.15)/ 10`$

### e)

```{r}
L = 1/10 * ((200.83/7.15) * (1/ (qf(.95, 3, 4*9))) -1)
U = 1/10 * ((200.83/7.15) * (1/ (qf(0.5, 3, 4*9))) -1)
```


$L = `r 1/10 * ((200.83/7.15) * (1/ (qf(.95, 3, 4*9))) -1)`$

$U = `r 1/10 * ((200.83/7.15) * (1/ (qf(0.5, 3, 4*9))) -1)`$


$L^* = `r L/(1+L)`$

$U^* = `r U / (1+U)`$


$`r L/(1+L)` \le \frac{\sigma_{\mu}^2}{\sigma_{\mu}^2 + \sigma^2} \le `r U / (1+U)`$

The result indicates that the variability in the measured characteristic in products from the four machines accounts for between 47 to 77 percent of the total variability in measure characteristics.


### f)

$H_0: \sigma_{\mu}^2 = \sigma^2$

$H_a: \sigma_{\mu}^2 \ne \sigma^2$


$F^* = `r (200.83/ (10 * 19.368 + 7.15)) / (7.15/7.15)`$

$F = `r qf(0.9, 3, 4*9)` > F^*$

concluding the $H_0$. 

also, $`r L` \le \sigma_{\mu} ^2 / \sigma^2 \le `r U`$, we can see that the 90% interval includes 1.